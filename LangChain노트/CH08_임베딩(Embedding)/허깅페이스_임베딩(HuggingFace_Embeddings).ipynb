{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:48.561874Z",
     "start_time": "2025-04-07T08:38:48.530624Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:48.579610Z",
     "start_time": "2025-04-07T08:38:48.574273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "3a97abd6d18505a4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 샘플 데이터",
   "id": "79de926d1af0d766"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:48.592432Z",
     "start_time": "2025-04-07T08:38:48.588924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "texts = [\n",
    "    \"안녕, 만나서 반가워.\",\n",
    "    \"LangChain simplifies the process of building applications with large language models\",\n",
    "    \"랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \",\n",
    "    \"LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
    "]"
   ],
   "id": "8d64bacaaede85d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HuggingFace Endpoint Embedding",
   "id": "c56e66ff2ede150e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:48.607331Z",
     "start_time": "2025-04-07T08:38:48.600846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=model_name,\n",
    "    task='feature-extraction',\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACE_API_TOKEN\"],\n",
    ")"
   ],
   "id": "ec3c2a08cf4ec03b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:49.159009Z",
     "start_time": "2025-04-07T08:38:48.628319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Document Embedding 수행\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)"
   ],
   "id": "66db54985b9f6f0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 525 ms\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:49.174536Z",
     "start_time": "2025-04-07T08:38:49.169922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"[HuggingFace Endpoint Embedding]\")\n",
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ],
   "id": "4f65ca38817840b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HuggingFace Endpoint Embedding]\n",
      "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:50.505003Z",
     "start_time": "2025-04-07T08:38:49.196027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Document Embedding 수행\n",
    "embedded_query = hf_embeddings.embed_query(\"LangChain에 대해서 알려주세요.\")"
   ],
   "id": "d3247cdfc1071dba",
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/pipeline/feature-extraction/intfloat/multilingual-e5-large-instruct",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m409\u001B[39m     \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/pipeline/feature-extraction/intfloat/multilingual-e5-large-instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mHfHubHTTPError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Document Embedding 수행\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m embedded_query = \u001B[43mhf_embeddings\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_query\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mLangChain에 대해서 알려주세요.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface_endpoint.py:143\u001B[39m, in \u001B[36mHuggingFaceEndpointEmbeddings.embed_query\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    134\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34membed_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) -> List[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[32m    135\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call out to HuggingFaceHub's embedding endpoint for embedding query text.\u001B[39;00m\n\u001B[32m    136\u001B[39m \n\u001B[32m    137\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    141\u001B[39m \u001B[33;03m        Embeddings for the text.\u001B[39;00m\n\u001B[32m    142\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface_endpoint.py:112\u001B[39m, in \u001B[36mHuggingFaceEndpointEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts)\u001B[39m\n\u001B[32m    110\u001B[39m _model_kwargs = \u001B[38;5;28mself\u001B[39m.model_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[32m    111\u001B[39m \u001B[38;5;66;03m#  api doc: https://huggingface.github.io/text-embeddings-inference/#/Text%20Embeddings%20Inference/embed\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m112\u001B[39m responses = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minputs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43m_model_kwargs\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtask\u001B[49m\n\u001B[32m    114\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m json.loads(responses.decode())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001B[39m, in \u001B[36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    130\u001B[39m     warning_message += \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m + message\n\u001B[32m    131\u001B[39m warnings.warn(warning_message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:268\u001B[39m, in \u001B[36mInferenceClient.post\u001B[39m\u001B[34m(self, json, data, model, task, stream)\u001B[39m\n\u001B[32m    266\u001B[39m url = provider_helper._prepare_url(\u001B[38;5;28mself\u001B[39m.token, mapped_model)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    267\u001B[39m headers = provider_helper._prepare_headers(\u001B[38;5;28mself\u001B[39m.headers, \u001B[38;5;28mself\u001B[39m.token)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m268\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inner_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    269\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest_parameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mRequestParameters\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    270\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43munknown\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    272\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43munknown\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    273\u001B[39m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[43m=\u001B[49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    274\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    275\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    276\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    277\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    278\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:321\u001B[39m, in \u001B[36mInferenceClient._inner_post\u001B[39m\u001B[34m(self, request_parameters, stream)\u001B[39m\n\u001B[32m    318\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest_parameters.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m    320\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m321\u001B[39m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    322\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response.iter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response.content\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AI_study\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:481\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    477\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    479\u001B[39m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[32m    480\u001B[39m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m481\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, \u001B[38;5;28mstr\u001B[39m(e), response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mHfHubHTTPError\u001B[39m: 503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/pipeline/feature-extraction/intfloat/multilingual-e5-large-instruct"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:50.509010400Z",
     "start_time": "2025-04-06T13:52:07.052589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
    "np.array(embedded_query) @ np.array(embedded_documents).T"
   ],
   "id": "b1ef58ae3d599f55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8428121 , 0.86560862, 0.86114489, 0.89703807, 0.77247177])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:50.520829300Z",
     "start_time": "2025-04-06T13:52:17.680620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]\n",
    "sorted_idx"
   ],
   "id": "22b9a400d47e54bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2, 0, 4], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:50.523766500Z",
     "start_time": "2025-04-06T13:52:42.832329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] {texts[idx]}\")\n",
    "    print()"
   ],
   "id": "b4b5ffef3a8a9593",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] LangChain 에 대해서 알려주세요.\n",
      "====================================\n",
      "[0] LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\n",
      "\n",
      "[1] LangChain simplifies the process of building applications with large language models\n",
      "\n",
      "[2] 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \n",
      "\n",
      "[3] 안녕, 만나서 반가워.\n",
      "\n",
      "[4] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HuggingFace Embeddings",
   "id": "62d6d30f767cefd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:39:03.880123Z",
     "start_time": "2025-04-07T08:38:56.507672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cpu\"},  # cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ],
   "id": "bdef5277c25f7d24",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:40:52.628485Z",
     "start_time": "2025-04-07T08:40:51.924569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%time\n",
    "# Document\n",
    "embedded_documents1 = hf_embeddings.embed_documents(texts)"
   ],
   "id": "e54b9f266fc4adda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:41:10.208764Z",
     "start_time": "2025-04-07T08:41:10.203862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ],
   "id": "6943da1fceb8fda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tintfloat/multilingual-e5-large-instruct\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BGE-M3 임베딩",
   "id": "eeef30f03ef22c2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:42:39.359592Z",
     "start_time": "2025-04-07T08:42:00.241276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ],
   "id": "e85012c1ddeebe9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3941d45c58614f81a84036aef044896b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58c400e3fb2349f4b7e142fce465987d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/15.8k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8222b61b64aa440eb184a962b557db4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a979e96cc4bb4c6895a77e137e649a29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cf310286aad4ab3aa6714c8989dc03b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0813cd7b0ac449158f22bed4d407718a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c923121d0ca4d758756e30232004b42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0b9ec333b254d23997b65d01752ed80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d30a43c33a8044ec8638b5c0b2c6b37f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a230960e66144e8b82a45e8126e8e159"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b13f10c489e04533b17cca2b6062795a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7be03b9af00743a081e481efb76b9d48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:42:40.386757Z",
     "start_time": "2025-04-07T08:42:39.364601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%time\n",
    "# Document\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)"
   ],
   "id": "1e37a3ac2e995953",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:42:40.402788Z",
     "start_time": "2025-04-07T08:42:40.396908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ],
   "id": "a34fc70cb408217e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tBAAI/bge-m3\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
