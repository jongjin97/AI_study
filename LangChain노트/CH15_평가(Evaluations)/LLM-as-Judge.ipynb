{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T06:36:48.499100Z",
     "start_time": "2025-04-16T06:36:48.481983Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:36:52.907562Z",
     "start_time": "2025-04-16T06:36:52.900760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH16-Evaluations\")"
   ],
   "id": "f7b404072df58955",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH16-Evaluations\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG 성능 테스트를 위한 함수 정의",
   "id": "60115e0e8edd275d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:38:29.899120Z",
     "start_time": "2025-04-16T06:38:22.993756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from myrag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# PDFRAG 객체 생성\n",
    "rag = PDFRAG(\n",
    "    \"data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    ")\n",
    "\n",
    "# 검색기(retriever) 생성\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "# 체인(chain) 생성\n",
    "chain = rag.create_chain(retriever)\n",
    "\n",
    "# 질문에 대한 답변 생성\n",
    "chain.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")"
   ],
   "id": "827c8c2d640cb225",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:38:29.983425Z",
     "start_time": "2025-04-16T06:38:29.979609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 질문에 대한 답변하는 함수를 생성\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ],
   "id": "384dfc07eb4a1db2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:38:48.751339Z",
     "start_time": "2025-04-16T06:38:46.759462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 사용자 질문 예시\n",
    "llm_answer = ask_question(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"}\n",
    ")\n",
    "llm_answer"
   ],
   "id": "c8b388d7422074c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:38:54.140254Z",
     "start_time": "2025-04-16T06:38:54.136932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluator prompt 출력을 위한 함수\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ],
   "id": "c86b717607017601",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Question-Answer Evaluator\n",
   "id": "713e580bc464c3aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:39:45.072908Z",
     "start_time": "2025-04-16T06:39:44.144026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# qa 평가자 생성\n",
    "qa_evalulator = LangChainStringEvaluator(\"qa\")\n",
    "\n",
    "# 프롬프트 출력\n",
    "print_evaluator_prompt(qa_evalulator)"
   ],
   "id": "e9f9c7336d41228e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001B[33;1m\u001B[1;3m{query}\u001B[0m\n",
      "STUDENT ANSWER: \u001B[33;1m\u001B[1;3m{result}\u001B[0m\n",
      "TRUE ANSWER: \u001B[33;1m\u001B[1;3m{answer}\u001B[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:40:29.200414Z",
     "start_time": "2025-04-16T06:40:10.175960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evalulator],\n",
    "    experiment_prefix=\"RAG_EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"QA Evaluator 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ],
   "id": "e2ddd500f3b100e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL-077c77b2' at:\n",
      "https://smith.langchain.com/o/9b141874-d093-4103-946d-7bc247255f98/datasets/899fb1c5-744d-4f35-a48e-68fe78d807f1/compare?selectedSessions=38ebb484-4b5f-486f-bdee-e8ad05a5f3f4\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbe231d16be54334a1926986aa8fe57f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Context 에 기반한 답변 Evaluator",
   "id": "bdf58f8004f779d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:42:44.760234Z",
     "start_time": "2025-04-16T06:42:44.754715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Context 를 반환하는 RAG 결과 반환 함수\n",
    "def context_answer_rag_answer(inputs: dict):\n",
    "    context = retriever.invoke(inputs[\"question\"])\n",
    "    return {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in context]),\n",
    "        \"answer\": chain.invoke(inputs[\"question\"]),\n",
    "        \"query\": inputs[\"question\"],\n",
    "    }"
   ],
   "id": "8ab9ca36ac726103",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:43:07.202648Z",
     "start_time": "2025-04-16T06:43:05.263213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 함수 실행\n",
    "context_answer_rag_answer(\n",
    "    {\"question\": \"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\"}\n",
    ")"
   ],
   "id": "106a91dce63014ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ··························································· 10\\n   ▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································ 11\\n   ▹ IDC, 2027년 AI 소프트웨어 매출 2,500억 달러 돌파 전망··········································· 12\\nSPRi AI Brief |  \\n2023-12월호\\n10\\n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\\nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성 \\nAI 모델 ‘삼성 가우스’를 공개\\nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한 \\n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유\\nKEY Contents\\n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원\\n£ 언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원\\nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델 \\n‘삼성 가우스’를 최초 공개\\n∙정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에 \\n최적화된 크기의 모델 선택이 가능\\n∙삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며, \\n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유\\n어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상\\n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.\\n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.\\nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.',\n",
       " 'answer': \"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\",\n",
       " 'query': '삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:43:50.457531Z",
     "start_time": "2025-04-16T06:43:49.241853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cot_qa 평가자 생성\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],  # LLM 이 생성한 답변\n",
    "        \"reference\": run.outputs[\"context\"],  # Context\n",
    "        \"input\": example.inputs[\"question\"],  # 데이터셋의 질문\n",
    "    },\n",
    ")\n",
    "\n",
    "# context_qa 평가자 생성\n",
    "context_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"context_qa\",\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],  # LLM 이 생성한 답변\n",
    "        \"reference\": run.outputs[\"context\"],  # Context\n",
    "        \"input\": example.inputs[\"question\"],  # 데이터셋의 질문\n",
    "    },\n",
    ")\n",
    "\n",
    "# evaluator prompt 출력\n",
    "print_evaluator_prompt(context_qa_evaluator)"
   ],
   "id": "f577f1d949e9cf71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, based on the context.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "CONTEXT: context the question is about here\n",
      "STUDENT ANSWER: student's answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001B[33;1m\u001B[1;3m{query}\u001B[0m\n",
      "CONTEXT: \u001B[33;1m\u001B[1;3m{context}\u001B[0m\n",
      "STUDENT ANSWER: \u001B[33;1m\u001B[1;3m{result}\u001B[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:44:42.306546Z",
     "start_time": "2025-04-16T06:44:02.351668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋 이름 설정\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "evaluate(\n",
    "    context_answer_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator, context_qa_evaluator],\n",
    "    experiment_prefix=\"RAG_EVAL\",\n",
    "    metadata={\n",
    "        \"variant\": \"COT_QA & Context_QA Evaluator 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ],
   "id": "bb715a0208f1ab1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG_EVAL-0ad28197' at:\n",
      "https://smith.langchain.com/o/9b141874-d093-4103-946d-7bc247255f98/datasets/899fb1c5-744d-4f35-a48e-68fe78d807f1/compare?selectedSessions=1b8ea546-6383-4764-a008-b193cbcf302b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f64cd172008b43c7b9e951c4605f3be3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults RAG_EVAL-0ad28197>"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.context</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.query</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>feedback.Contextual Accuracy</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?</td>\n",
       "      <td>£ 구글, 앤스로픽에 최대 20억 달러 투자 합의 및 클라우드 서비스 제공\\nn 구...</td>\n",
       "      <td>구글이 앤스로픽에 최대 20억 달러를 투자하기로 합의한 것이 사실입니다. 테디노트에...</td>\n",
       "      <td>구글이 테디노트에게 20억달러를 투자한 것이 사실입니까?</td>\n",
       "      <td>None</td>\n",
       "      <td>사실이 아닙니다. 구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.976184</td>\n",
       "      <td>38dd13b9-f29d-4eb1-a819-a9d3a5c3a6b1</td>\n",
       "      <td>95ae25b9-d867-47c0-85b5-ba66ab60d879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ··············...</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 '삼성 가우스'입니다.</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>None</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 테디노트 입니다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.011025</td>\n",
       "      <td>d36a6bbf-4242-402c-bb79-c391fae56441</td>\n",
       "      <td>64ff9dee-d653-495d-8c1f-a6d0dfcfd764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>코히어의 데이터 출처 탐색기에 대해서 간략히 말해주세요.</td>\n",
       "      <td>SPRi AI Brief |  \\n2023-12월호\\n8\\n코히어, 데이터 투명성 ...</td>\n",
       "      <td>코히어는 2023년 10월 25일에 '데이터 출처 탐색기(Data Provenanc...</td>\n",
       "      <td>코히어의 데이터 출처 탐색기에 대해서 간략히 말해주세요.</td>\n",
       "      <td>None</td>\n",
       "      <td>코히어의 데이터 출처 탐색기는 AI 모델 훈련에 사용되는 데이터셋의 출처와 라이선스...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.641142</td>\n",
       "      <td>c5ee731d-c5a1-447f-9462-61b84f25e7ba</td>\n",
       "      <td>d1da3674-8da3-4c79-8e88-46c834d0a698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>미국 바이든 대통령이 안전하고 신뢰할 수 있는 AI 개발과 사용을 보장하기 위한 행...</td>\n",
       "      <td>1. 정책/법제  \\n2. 기업/산업 \\n3. 기술/연구 \\n 4. 인력/교육\\n미...</td>\n",
       "      <td>2023년 10월 30일입니다.</td>\n",
       "      <td>미국 바이든 대통령이 안전하고 신뢰할 수 있는 AI 개발과 사용을 보장하기 위한 행...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023년 10월 30일 미국 바이든 대통령이 행정명령을 발표했습니다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.607767</td>\n",
       "      <td>e2350266-cdc9-402c-bc99-4ccdd2645d6a</td>\n",
       "      <td>6e29f3d4-b91c-4e9a-89ad-acf4b3108dd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ··············...</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 '삼성 가우스'입니다.</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 무엇인가요?</td>\n",
       "      <td>None</td>\n",
       "      <td>삼성전자가 만든 생성형 AI의 이름은 삼성 가우스 입니다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.636177</td>\n",
       "      <td>ece662f4-c811-4101-b60f-1478680f1c0a</td>\n",
       "      <td>fbefdf7e-ae82-41d7-92b0-f7da8e29c5b3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Criteria",
   "id": "30a9dd32147f515b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:49:58.776046Z",
     "start_time": "2025-04-16T06:48:35.947033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# 평가자 설정\n",
    "criteria_evaluator = [\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"conciseness\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"misogyny\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"criminality\"}),\n",
    "]\n",
    "\n",
    "# 데이터셋 이름 설정\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=criteria_evaluator,\n",
    "    experiment_prefix=\"CRITERIA-EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"criteria 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ],
   "id": "7a2b231de8105ee9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'CRITERIA-EVAL-eb6fa56d' at:\n",
      "https://smith.langchain.com/o/9b141874-d093-4103-946d-7bc247255f98/datasets/899fb1c5-744d-4f35-a48e-68fe78d807f1/compare?selectedSessions=79e8c568-2a15-4a59-a7e0-6aa72e9cad51\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fa43f13fa734729b9c8fee5153c7464"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 정답이 존재하는 경우 Evaluator 활용(labeled_criteria)",
   "id": "7322a651abc3f195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:53:15.537731Z",
     "start_time": "2025-04-16T06:53:14.895029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# labeled_criteria 평가자 생성\n",
    "labeled_criteria_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"helpfulness\": (\n",
    "                \"Is this submission helpfull to the user,\"\n",
    "                \" taking into account the correct reference answer?\"\n",
    "            )\n",
    "        },\n",
    "        \"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# evaluator prompt 출력\n",
    "print_evaluator_prompt(labeled_criteria_evaluator)"
   ],
   "id": "d4a1a1d0e34d9a94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
      "[BEGIN DATA]\n",
      "***\n",
      "[Input]: \u001B[33;1m\u001B[1;3m{input}\u001B[0m\n",
      "***\n",
      "[Submission]: \u001B[33;1m\u001B[1;3m{output}\u001B[0m\n",
      "***\n",
      "[Criteria]: helpfulness: Is this submission helpfull to the user, taking into account the correct reference answer?\n",
      "***\n",
      "[Reference]: \u001B[33;1m\u001B[1;3m{reference}\u001B[0m\n",
      "***\n",
      "[END DATA]\n",
      "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:53:27.210785Z",
     "start_time": "2025-04-16T06:53:26.440096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "relevance_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": \"relevance\",\n",
    "        \"llm\": ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\"),\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],  # Context 를 전달\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_evaluator_prompt(relevance_evaluator)"
   ],
   "id": "84a8169d8d7b3177",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
      "[BEGIN DATA]\n",
      "***\n",
      "[Input]: \u001B[33;1m\u001B[1;3m{input}\u001B[0m\n",
      "***\n",
      "[Submission]: \u001B[33;1m\u001B[1;3m{output}\u001B[0m\n",
      "***\n",
      "[Criteria]: relevance: Is the submission referring to a real quote from the text?\n",
      "***\n",
      "[Reference]: \u001B[33;1m\u001B[1;3m{reference}\u001B[0m\n",
      "***\n",
      "[END DATA]\n",
      "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:55:04.296744Z",
     "start_time": "2025-04-16T06:53:55.193019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 데이터셋 이름 설정\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    context_answer_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[labeled_criteria_evaluator, relevance_evaluator],\n",
    "    experiment_prefix=\"LABELED-EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"labeled_criteria evaluator 활용한 평가\",\n",
    "    },\n",
    ")"
   ],
   "id": "efe1fd06f4b0662d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'LABELED-EVAL-bca7d781' at:\n",
      "https://smith.langchain.com/o/9b141874-d093-4103-946d-7bc247255f98/datasets/899fb1c5-744d-4f35-a48e-68fe78d807f1/compare?selectedSessions=9f47c6ab-8def-4fc7-ada8-34891a2b9b5b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "637d7adbc9cd4f98b5af23484b9d43d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 사용자 정의 점수 Evaluator(labeled_score_string)",
   "id": "4d659f0eeffdc429"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:55:05.028476Z",
     "start_time": "2025-04-16T06:55:04.343752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "# 점수를 반환하는 평가자 생성\n",
    "labeled_score_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "        \"llm\": ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\"),\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print_evaluator_prompt(labeled_score_evaluator)"
   ],
   "id": "561b42ae0294f93a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m System Message \u001B[0m================================\n",
      "\n",
      "You are a helpful assistant.\n",
      "\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "[Instruction]\n",
      "Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. \u001B[33;1m\u001B[1;3m{criteria}\u001B[0m[Ground truth]\n",
      "\u001B[33;1m\u001B[1;3m{reference}\u001B[0m\n",
      "Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n",
      "\n",
      "[Question]\n",
      "\u001B[33;1m\u001B[1;3m{input}\u001B[0m\n",
      "\n",
      "[The Start of Assistant's Answer]\n",
      "\u001B[33;1m\u001B[1;3m{prediction}\u001B[0m\n",
      "[The End of Assistant's Answer]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:56:22.203546Z",
     "start_time": "2025-04-16T06:55:55.977244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[labeled_score_evaluator],\n",
    "    experiment_prefix=\"LABELED-SCORE-EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"labeled_score 활용한 평가\",\n",
    "    },\n",
    ")"
   ],
   "id": "5407ba45feaf7c27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'LABELED-SCORE-EVAL-51e3c9d4' at:\n",
      "https://smith.langchain.com/o/9b141874-d093-4103-946d-7bc247255f98/datasets/899fb1c5-744d-4f35-a48e-68fe78d807f1/compare?selectedSessions=968f0652-a53d-438d-88ea-f6df9bde6813\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa91ba640fe34ec198d6e4e58f117594"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
