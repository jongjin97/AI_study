{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T08:38:42.142882Z",
     "start_time": "2025-04-16T08:38:42.125920Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH16-Evaluations\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH16-Evaluations\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG 성능 테스트를 위한 함수 정의",
   "id": "285ce48efeaec032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:39:16.234305Z",
     "start_time": "2025-04-16T08:39:04.043217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from myrag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# PDFRAG 객체 생성\n",
    "rag = PDFRAG(\n",
    "    \"data/SPRI_AI_Brief_2023년12월호_F.pdf\",\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    ")\n",
    "\n",
    "# 검색기(retriever) 생성\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "# 체인(chain) 생성\n",
    "chain = rag.create_chain(retriever)\n",
    "\n",
    "# 질문에 대한 답변 생성\n",
    "chain.invoke(\"삼성전자가 자체 개발한 생성형 AI의 이름은 무엇인가요?\")"
   ],
   "id": "e6a9ab7b1c9e683",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"삼성전자가 자체 개발한 생성형 AI의 이름은 '삼성 가우스'입니다.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:39:16.310416Z",
     "start_time": "2025-04-16T08:39:16.303899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 질문에 대한 답변하는 함수를 생성\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ],
   "id": "b61b381fdd0685ef",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 한글 형태소 분석기의 활용",
   "id": "d588d21132ae06ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:39:45.265418Z",
     "start_time": "2025-04-16T08:39:33.744032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_teddynote.community.kiwi_tokenizer import KiwiTokenizer\n",
    "\n",
    "# 토크나이저 선언\n",
    "kiwi_tokenizer = KiwiTokenizer()\n",
    "\n",
    "sent1 = \"안녕하세요. 반갑습니다. 내 이름은 테디입니다.\"\n",
    "sent2 = \"안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\"\n",
    "\n",
    "# 토큰화\n",
    "print(sent1.split())\n",
    "print(sent2.split())\n",
    "\n",
    "print(\"===\" * 20)\n",
    "\n",
    "# 토큰화\n",
    "print(kiwi_tokenizer.tokenize(sent1))\n",
    "print(kiwi_tokenizer.tokenize(sent2))"
   ],
   "id": "8b49da722db9dc50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요.', '반갑습니다.', '내', '이름은', '테디입니다.']\n",
      "['안녕하세용', '반갑습니다~^^', '내', '이름은', '테디입니다!!']\n",
      "============================================================\n",
      "['안녕', '하', '세요', '.', '반갑', '습니다', '.', '나', '의', '이름', '은', '테디', '이', 'ᆸ니다', '.']\n",
      "['안녕', '하', '세요', 'ᆼ', '반갑', '습니다', '~', '^^', '나', '의', '이름', '은', '테디', '이', 'ᆸ니다', '!!']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Rouge (Recall-Oriented Understudy for Gisting Evaluation) 스코어",
   "id": "e19753fcfed05ce9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:41:29.056451Z",
     "start_time": "2025-04-16T08:41:26.694395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "sent1 = \"안녕하세요. 반갑습니다. 내 이름은 테디입니다.\"\n",
    "sent2 = \"안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\"\n",
    "sent3 = \"내 이름은 테디입니다. 안녕하세요. 반갑습니다.\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=False, tokenizer=KiwiTokenizer()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"[1] {sent1}\\n[2] {sent2}\\n[rouge1] {scorer.score(sent1, sent2)['rouge1'].fmeasure:.5f}\\n[rouge2] {scorer.score(sent1, sent2)['rouge2'].fmeasure:.5f}\\n[rougeL] {scorer.score(sent1, sent2)['rougeL'].fmeasure:.5f}\"\n",
    ")\n",
    "print(\"===\" * 20)\n",
    "print(\n",
    "    f\"[1] {sent1}\\n[2] {sent3}\\n[rouge1] {scorer.score(sent1, sent3)['rouge1'].fmeasure:.5f}\\n[rouge2] {scorer.score(sent1, sent3)['rouge2'].fmeasure:.5f}\\n[rougeL] {scorer.score(sent1, sent3)['rougeL'].fmeasure:.5f}\")"
   ],
   "id": "5998cf1979279656",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\n",
      "[rouge1] 0.77419\n",
      "[rouge2] 0.62069\n",
      "[rougeL] 0.77419\n",
      "============================================================\n",
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 내 이름은 테디입니다. 안녕하세요. 반갑습니다.\n",
      "[rouge1] 1.00000\n",
      "[rouge2] 0.92857\n",
      "[rougeL] 0.53333\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BLEU (Bilingual Evaluation Understudy) 스코어",
   "id": "5b8fe5ea8b6c5c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:57:01.224460Z",
     "start_time": "2025-04-16T08:57:01.216784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "sent1 = \"안녕하세요. 반갑습니다. 내 이름은 테디입니다.\"\n",
    "sent2 = \"안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\"\n",
    "sent3 = \"내 이름은 테디입니다. 안녕하세요. 반갑습니다.\"\n",
    "\n",
    "# 토큰화\n",
    "print(kiwi_tokenizer.tokenize(sent1, type=\"sentence\"))\n",
    "print(kiwi_tokenizer.tokenize(sent2, type=\"sentence\"))\n",
    "print(kiwi_tokenizer.tokenize(sent3, type=\"sentence\"))"
   ],
   "id": "36f9167d2ed583ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕 하 세요 . 반갑 습니다 . 나 의 이름 은 테디 이 ᆸ니다 .\n",
      "안녕 하 세요 ᆼ 반갑 습니다 ~ ^^ 나 의 이름 은 테디 이 ᆸ니다 !!\n",
      "나 의 이름 은 테디 이 ᆸ니다 . 안녕 하 세요 . 반갑 습니다 .\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:57:21.447578Z",
     "start_time": "2025-04-16T08:57:21.437521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bleu_score = sentence_bleu(\n",
    "    [kiwi_tokenizer.tokenize(sent1, type=\"sentence\")],\n",
    "    kiwi_tokenizer.tokenize(sent2, type=\"sentence\"),\n",
    ")\n",
    "print(f\"[1] {sent1}\\n[2] {sent2}\\n[score] {bleu_score:.5f}\")\n",
    "print(\"===\" * 20)\n",
    "\n",
    "bleu_score = sentence_bleu(\n",
    "    [kiwi_tokenizer.tokenize(sent1, type=\"sentence\")],\n",
    "    kiwi_tokenizer.tokenize(sent3, type=\"sentence\"),\n",
    ")\n",
    "print(f\"[1] {sent1}\\n[2] {sent3}\\n[score] {bleu_score:.5f}\")"
   ],
   "id": "1260abab4e54d887",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\n",
      "[score] 0.74879\n",
      "============================================================\n",
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 내 이름은 테디입니다. 안녕하세요. 반갑습니다.\n",
      "[score] 0.95739\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SemScore",
   "id": "5f03fa2c694a0871"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:59:49.436880Z",
     "start_time": "2025-04-16T08:59:22.239982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sent1 = \"안녕하세요. 반갑습니다. 내 이름은 테디입니다.\"\n",
    "sent2 = \"안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\"\n",
    "sent3 = \"내 이름은 테디입니다. 안녕하세요. 반갑습니다.\"\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# 문장들을 인코딩\n",
    "sent1_encoded = model.encode(sent1, convert_to_tensor=True)\n",
    "sent2_encoded = model.encode(sent2, convert_to_tensor=True)\n",
    "sent3_encoded = model.encode(sent3, convert_to_tensor=True)\n",
    "\n",
    "# sent1과 sent2 사이의 코사인 유사도 계산\n",
    "cosine_similarity = util.pytorch_cos_sim(sent1_encoded, sent2_encoded).item()\n",
    "print(f\"[1] {sent1}\\n[2] {sent2}\\n[score] {cosine_similarity:.5f}\")\n",
    "\n",
    "print(\"===\" * 20)\n",
    "\n",
    "# sent1과 sent3 사이의 코사인 유사도 계산\n",
    "cosine_similarity = util.pytorch_cos_sim(sent1_encoded, sent3_encoded).item()\n",
    "print(f\"[1] {sent1}\\n[2] {sent3}\\n[score] {cosine_similarity:.5f}\")"
   ],
   "id": "f87969e6228d3e73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 안녕하세용 반갑습니다~^^ 내 이름은 테디입니다!!\n",
      "[score] 0.86157\n",
      "============================================================\n",
      "[1] 안녕하세요. 반갑습니다. 내 이름은 테디입니다.\n",
      "[2] 내 이름은 테디입니다. 안녕하세요. 반갑습니다.\n",
      "[score] 0.99191\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:00:05.105593Z",
     "start_time": "2025-04-16T09:00:05.094608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate import meteor_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "\n",
    "# 토크나이저 병렬화 설정(HuggingFace 모델 사용)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "\n",
    "def rouge_evaluator(metric: str = \"rouge1\") -> dict:\n",
    "    # wrapper function 정의\n",
    "    def _rouge_evaluator(run: Run, example: Example) -> dict:\n",
    "        # 출력값과 정답 가져오기\n",
    "        student_answer = run.outputs.get(\"answer\", \"\")\n",
    "        reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "        # ROUGE 점수 계산\n",
    "        scorer = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True, tokenizer=KiwiTokenizer()\n",
    "        )\n",
    "        scores = scorer.score(reference_answer, student_answer)\n",
    "\n",
    "        # ROUGE 점수 반환\n",
    "        rouge = scores[metric].fmeasure\n",
    "\n",
    "        return {\"key\": \"ROUGE\", \"score\": rouge}\n",
    "\n",
    "    return _rouge_evaluator\n",
    "\n",
    "\n",
    "def bleu_evaluator(run: Run, example: Example) -> dict:\n",
    "    # 출력값과 정답 가져오기\n",
    "    student_answer = run.outputs.get(\"answer\", \"\")\n",
    "    reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "    # 토큰화\n",
    "    reference_tokens = kiwi_tokenizer.tokenize(reference_answer, type=\"sentence\")\n",
    "    student_tokens = kiwi_tokenizer.tokenize(student_answer, type=\"sentence\")\n",
    "\n",
    "    # BLEU 점수 계산\n",
    "    bleu_score = sentence_bleu([reference_tokens], student_tokens)\n",
    "\n",
    "    return {\"key\": \"BLEU\", \"score\": bleu_score}\n",
    "\n",
    "\n",
    "def meteor_evaluator(run: Run, example: Example) -> dict:\n",
    "    # 출력값과 정답 가져오기\n",
    "    student_answer = run.outputs.get(\"answer\", \"\")\n",
    "    reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "    # 토큰화\n",
    "    reference_tokens = kiwi_tokenizer.tokenize(reference_answer, type=\"list\")\n",
    "    student_tokens = kiwi_tokenizer.tokenize(student_answer, type=\"list\")\n",
    "\n",
    "    # METEOR 점수 계산\n",
    "    meteor = meteor_score.meteor_score([reference_tokens], student_tokens)\n",
    "\n",
    "    return {\"key\": \"METEOR\", \"score\": meteor}\n",
    "\n",
    "\n",
    "def semscore_evaluator(run: Run, example: Example) -> dict:\n",
    "    # 출력값과 정답 가져오기\n",
    "    student_answer = run.outputs.get(\"answer\", \"\")\n",
    "    reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "    # SentenceTransformer 모델 로드\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "    # 문장 임베딩 생성\n",
    "    student_embedding = model.encode(student_answer, convert_to_tensor=True)\n",
    "    reference_embedding = model.encode(reference_answer, convert_to_tensor=True)\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    cosine_similarity = util.pytorch_cos_sim(\n",
    "        student_embedding, reference_embedding\n",
    "    ).item()\n",
    "\n",
    "    return {\"key\": \"sem_score\", \"score\": cosine_similarity}\n"
   ],
   "id": "cd7c2d02c665e2a7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:01:06.297517Z",
     "start_time": "2025-04-16T09:00:18.565148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 평가자 정의\n",
    "heuristic_evalulators = [\n",
    "    rouge_evaluator(metric=\"rougeL\"),\n",
    "    bleu_evaluator,\n",
    "    meteor_evaluator,\n",
    "    semscore_evaluator,\n",
    "]\n",
    "\n",
    "# 데이터셋 이름 설정\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "# 실험 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=heuristic_evalulators,\n",
    "    experiment_prefix=\"Heuristic-EVAL\",\n",
    "    # 실험 메타데이터 지정\n",
    "    metadata={\n",
    "        \"variant\": \"Heuristic-EVAL (Rouge, BLEU, METEOR, SemScore) 을 사용하여 평가\",\n",
    "    },\n",
    ")\n"
   ],
   "id": "3f93d36a62ad15fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Heuristic-EVAL-c903d355' at:\n",
      "https://smith.langchain.com/o/9b141874-d093-4103-946d-7bc247255f98/datasets/899fb1c5-744d-4f35-a48e-68fe78d807f1/compare?selectedSessions=14722654-96be-4539-852f-a2ac67c91894\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f67edd1944b4491835b59a361f930d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator meteor_evaluator> on run 6045be35-256b-4345-8188-0bbd25b2ea09: LookupError(\"\\n**********************************************************************\\n  Resource \\x1b[93mwordnet\\x1b[0m not found.\\n  Please use the NLTK Downloader to obtain the resource:\\n\\n  \\x1b[31m>>> import nltk\\n  >>> nltk.download('wordnet')\\n  \\x1b[0m\\n  For more information see: https://www.nltk.org/data.html\\n\\n  Attempted to load \\x1b[93mcorpora/wordnet\\x1b[0m\\n\\n  Searched in:\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao/nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\share\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\lib\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\nltk_data'\\n    - 'D:\\\\\\\\nltk_data'\\n    - 'E:\\\\\\\\nltk_data'\\n**********************************************************************\\n\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet.zip/wordnet/\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 634, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 631, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AppData\\Local\\Temp\\ipykernel_25148\\1819025130.py\", line 58, in meteor_evaluator\n",
      "    meteor = meteor_score.meteor_score([reference_tokens], student_tokens)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 397, in meteor_score\n",
      "    return max(\n",
      "           ^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 398, in <genexpr>\n",
      "    single_meteor_score(\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 331, in single_meteor_score\n",
      "    matches, _, _ = _enum_align_words(\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 223, in _enum_align_words\n",
      "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
      "                                                             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 161, in _enum_wordnetsyn_match\n",
      "    for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 120, in __getattr__\n",
      "    self.__load()\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Error running evaluator <DynamicRunEvaluator meteor_evaluator> on run 13c961d0-5ddb-4ad5-9511-12d24fe9422b: LookupError(\"\\n**********************************************************************\\n  Resource \\x1b[93mwordnet\\x1b[0m not found.\\n  Please use the NLTK Downloader to obtain the resource:\\n\\n  \\x1b[31m>>> import nltk\\n  >>> nltk.download('wordnet')\\n  \\x1b[0m\\n  For more information see: https://www.nltk.org/data.html\\n\\n  Attempted to load \\x1b[93mcorpora/wordnet\\x1b[0m\\n\\n  Searched in:\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao/nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\share\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\lib\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\nltk_data'\\n    - 'D:\\\\\\\\nltk_data'\\n    - 'E:\\\\\\\\nltk_data'\\n**********************************************************************\\n\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet.zip/wordnet/\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 634, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 631, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AppData\\Local\\Temp\\ipykernel_25148\\1819025130.py\", line 58, in meteor_evaluator\n",
      "    meteor = meteor_score.meteor_score([reference_tokens], student_tokens)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 397, in meteor_score\n",
      "    return max(\n",
      "           ^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 398, in <genexpr>\n",
      "    single_meteor_score(\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 331, in single_meteor_score\n",
      "    matches, _, _ = _enum_align_words(\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 223, in _enum_align_words\n",
      "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
      "                                                             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 161, in _enum_wordnetsyn_match\n",
      "    for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 120, in __getattr__\n",
      "    self.__load()\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Error running evaluator <DynamicRunEvaluator meteor_evaluator> on run 93762c55-3cc6-4860-a057-114b5596e222: LookupError(\"\\n**********************************************************************\\n  Resource \\x1b[93mwordnet\\x1b[0m not found.\\n  Please use the NLTK Downloader to obtain the resource:\\n\\n  \\x1b[31m>>> import nltk\\n  >>> nltk.download('wordnet')\\n  \\x1b[0m\\n  For more information see: https://www.nltk.org/data.html\\n\\n  Attempted to load \\x1b[93mcorpora/wordnet\\x1b[0m\\n\\n  Searched in:\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao/nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\share\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\lib\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\nltk_data'\\n    - 'D:\\\\\\\\nltk_data'\\n    - 'E:\\\\\\\\nltk_data'\\n**********************************************************************\\n\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet.zip/wordnet/\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 634, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 631, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AppData\\Local\\Temp\\ipykernel_25148\\1819025130.py\", line 58, in meteor_evaluator\n",
      "    meteor = meteor_score.meteor_score([reference_tokens], student_tokens)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 397, in meteor_score\n",
      "    return max(\n",
      "           ^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 398, in <genexpr>\n",
      "    single_meteor_score(\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 331, in single_meteor_score\n",
      "    matches, _, _ = _enum_align_words(\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 223, in _enum_align_words\n",
      "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
      "                                                             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 161, in _enum_wordnetsyn_match\n",
      "    for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 120, in __getattr__\n",
      "    self.__load()\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Error running evaluator <DynamicRunEvaluator meteor_evaluator> on run cda4531b-8ee5-4256-88a6-b71bd51e7690: LookupError(\"\\n**********************************************************************\\n  Resource \\x1b[93mwordnet\\x1b[0m not found.\\n  Please use the NLTK Downloader to obtain the resource:\\n\\n  \\x1b[31m>>> import nltk\\n  >>> nltk.download('wordnet')\\n  \\x1b[0m\\n  For more information see: https://www.nltk.org/data.html\\n\\n  Attempted to load \\x1b[93mcorpora/wordnet\\x1b[0m\\n\\n  Searched in:\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao/nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\share\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\lib\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\nltk_data'\\n    - 'D:\\\\\\\\nltk_data'\\n    - 'E:\\\\\\\\nltk_data'\\n**********************************************************************\\n\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet.zip/wordnet/\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 634, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 631, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AppData\\Local\\Temp\\ipykernel_25148\\1819025130.py\", line 58, in meteor_evaluator\n",
      "    meteor = meteor_score.meteor_score([reference_tokens], student_tokens)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 397, in meteor_score\n",
      "    return max(\n",
      "           ^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 398, in <genexpr>\n",
      "    single_meteor_score(\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 331, in single_meteor_score\n",
      "    matches, _, _ = _enum_align_words(\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 223, in _enum_align_words\n",
      "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
      "                                                             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 161, in _enum_wordnetsyn_match\n",
      "    for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 120, in __getattr__\n",
      "    self.__load()\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Error running evaluator <DynamicRunEvaluator meteor_evaluator> on run 25c73912-790c-4669-bac7-d0dcdb266238: LookupError(\"\\n**********************************************************************\\n  Resource \\x1b[93mwordnet\\x1b[0m not found.\\n  Please use the NLTK Downloader to obtain the resource:\\n\\n  \\x1b[31m>>> import nltk\\n  >>> nltk.download('wordnet')\\n  \\x1b[0m\\n  For more information see: https://www.nltk.org/data.html\\n\\n  Attempted to load \\x1b[93mcorpora/wordnet\\x1b[0m\\n\\n  Searched in:\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao/nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\share\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AI_study\\\\\\\\.venv\\\\\\\\lib\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\Users\\\\\\\\kakao\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\nltk_data'\\n    - 'C:\\\\\\\\nltk_data'\\n    - 'D:\\\\\\\\nltk_data'\\n    - 'E:\\\\\\\\nltk_data'\\n**********************************************************************\\n\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet.zip/wordnet/\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 346, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 634, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 631, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AppData\\Local\\Temp\\ipykernel_25148\\1819025130.py\", line 58, in meteor_evaluator\n",
      "    meteor = meteor_score.meteor_score([reference_tokens], student_tokens)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 397, in meteor_score\n",
      "    return max(\n",
      "           ^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 398, in <genexpr>\n",
      "    single_meteor_score(\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 331, in single_meteor_score\n",
      "    matches, _, _ = _enum_align_words(\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 223, in _enum_align_words\n",
      "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
      "                                                             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\translate\\meteor_score.py\", line 161, in _enum_wordnetsyn_match\n",
      "    for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 120, in __getattr__\n",
      "    self.__load()\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kakao\\AI_study\\.venv\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001B[93mwordnet\u001B[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001B[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001B[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001B[93mcorpora/wordnet\u001B[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\kakao/nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AI_study\\\\.venv\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\kakao\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
